{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "huggingface.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORHa9hsozuOTkwl0eZoiL8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljhust/KaC/blob/master/templates/huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwmfWTkeH9Dx",
        "colab_type": "text"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfSkw65epceJ",
        "colab_type": "text"
      },
      "source": [
        "#### 1 simple invocation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlcttFN8H2pZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorflow==2.1.0\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Store the model we want to use\n",
        "MODEL_NAME = \"bert-base-cased\"\n",
        "\n",
        "# We need to create the model and tokenizer\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Tokens comes from a process that splits the input into sub-entities with interesting linguistic properties. \n",
        "tokens = tokenizer.tokenize(\"This is an input example\")\n",
        "print(\"Tokens: {}\".format(tokens))\n",
        "\n",
        "# This is not sufficient for the model, as it requires integers as input, \n",
        "# not a problem, let's convert tokens to ids.\n",
        "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokens id: {}\".format(tokens_ids))\n",
        "\n",
        "# Add the required special tokens\n",
        "tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
        "\n",
        "# We need to convert to a Deep Learning framework specific format, let's use PyTorch for now.\n",
        "tokens_pt = torch.tensor([tokens_ids])\n",
        "print(\"Tokens PyTorch: {}\".format(tokens_pt))\n",
        "\n",
        "# Now we're ready to go through BERT with out input\n",
        "# outputs is a token based rep for whole seq, which has size of (1, n_tokens, d_rep), used for NER like task.\n",
        "# pooled is a context based rep for seq, which has size of (1, d_rep), as a aggregated rep, used for classification task.\n",
        "outputs, pooled = model(tokens_pt)\n",
        "print(\"Token wise output: {}, Pooled output: {}\".format(outputs.shape, pooled.shape))\n",
        "\n",
        "# tokens = tokenizer.tokenize(\"This is an input example\")\n",
        "# tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "# tokens_pt = torch.tensor([tokens_ids])\n",
        "\n",
        "# This code can be factored into one-line as follow\n",
        "tokens_pt2 = tokenizer.encode_plus(\"This is an input example\", return_tensors=\"pt\")\n",
        "\n",
        "for key, value in tokens_pt2.items():\n",
        "    print(\"{}:\\n\\t{}\".format(key, value))\n",
        "\n",
        "outputs2, pooled2 = model(**tokens_pt2)\n",
        "print(\"Difference with previous code: ({}, {})\".format((outputs2 - outputs).sum(), (pooled2 - pooled).sum()))\n",
        "\n",
        "# Single segment input\n",
        "single_seg_input = tokenizer.encode_plus(\"This is a sample input\")\n",
        "\n",
        "# Multiple segment input\n",
        "multi_seg_input = tokenizer.encode_plus(\"This is segment A\", \"This is segment B\")\n",
        "\n",
        "# Let's load a BERT model for TensorFlow and PyTorch\n",
        "model_tf = TFBertModel.from_pretrained('bert-base-cased')\n",
        "model_pt = BertModel.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBzqJhLHsbej",
        "colab_type": "text"
      },
      "source": [
        "#### 2 ditillation\n",
        "With the goal of making Transformer-based NLP accessible to everyone we @huggingface developed models that take advantage of a training process called Distillation which allows us to drastically reduce the resources needed to run such models with almost zero drop in performances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ufgPFS4sehD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import DistilBertModel\n",
        "\n",
        "bert_distil = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
        "input_pt = tokenizer.encode_plus(\n",
        "    'This is a sample input to demonstrate performance of distiled models especially inference time', \n",
        "    return_tensors=\"pt\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}